# AUTOGENERATED! DO NOT EDIT! File to edit: 05_Methods_and_Experiments.ipynb (unless otherwise specified).

__all__ = ['History', 'NN', 'XGBoost', 'StepCalibrator', 'RandomForest', 'PlattCalibrator', 'weighted_quantile',
           'Method', 'LocalMethod', 'GroupAwareGlobal',"FrustratinglyEasyDomainAdaptation"]

# Cell
import os
os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"]="1,2,3"

# Cell
import tensorflow as tf
from tensorflow.keras import models
from tensorflow.keras import layers
from tensorflow import keras
from tqdm import tqdm,trange
import numpy as np
import pickle
from glob import glob

class History:
    def __init__(self,history):
        self.history = history

class NN(object):
    def __init__(self,*args,**kwargs):
        super().__init__(*args,**kwargs)

    @staticmethod
    def getHidden(hsize,inputs,nHidden=3):
        hiddenlayers = [(layers.Dense(hsize),
                         layers.LayerNormalization(),
                         layers.ReLU(),
                         layers.Dropout(.5)) for hiddenlayer in range(nHidden)]
        hiddenlayers = [l for hl in hiddenlayers for l in hl]
        hiddenlayers.append(layers.Dense(1,"sigmoid"))
        outputs = keras.models.Sequential(hiddenlayers)(inputs)
        return outputs

    @staticmethod
    def getModel(inputshape,hsize=128,nHidden=3):
        inputs = layers.Input(shape=inputshape[1])
        outputs = NN.getHidden(hsize,inputs,nHidden=nHidden)
        model = keras.models.Model(inputs=inputs,
                                   outputs=outputs)
        return model

    @staticmethod
    def trainModel(xTrain,yTrain,xVal,yVal,hsize,quiet=False,NEpochs=100,
               EarlyStopping=False,nHidden=3,EarlyStoppingPatience=20,
               optimizer=tf.keras.optimizers.SGD(1e-3,),batch_size=1024):
        clf = NN.getModel(xTrain.shape,hsize=hsize,nHidden=nHidden)
        clf.compile(optimizer=optimizer,
                    loss=keras.losses.BinaryCrossentropy(from_logits=False),
                    metrics=[keras.metrics.AUC(from_logits=False,name="auc")])
        if EarlyStopping:
            callbacks = [tf.keras.callbacks.EarlyStopping(patience=EarlyStoppingPatience,
                                                                 restore_best_weights=True)]
        else:
            callbacks = []
        if NEpochs < 3:
            history = clf.fit(xTrain,yTrain,validation_data=(xVal,yVal),validation_freq=[2],
                              epochs=NEpochs,batch_size=batch_size,verbose=not quiet)
        else:
            history = clf.fit(xTrain,yTrain,validation_data=(xVal,yVal),epochs=NEpochs,batch_size=batch_size,
                              callbacks=callbacks,verbose=not quiet)
        return clf,history

    def fit(self,xTrain,yTrain,validation_data=None,
            h_range=[32,64,128,256,512,1024],
            NEpochs=500,probEarlyStopping=1,
            nHiddenRange=np.arange(2,6),
            batch_size=1024,
            nModels=25,
            optimizers=[("adam",lambda: tf.keras.optimizers.Adam()),
                        ("RMSProp",lambda: tf.keras.optimizers.RMSprop())]):
        losses = np.zeros(nModels)
        models = []
        histories = []
        for i in trange(nModels):
            h = np.random.choice(h_range)
            nHidden = np.random.choice(nHiddenRange)
            earlyStopping = bool(np.random.binomial(1,probEarlyStopping))
            optimizerName,optimizerFactory = optimizers[np.random.randint(0,len(optimizers))]
            optimizer= optimizerFactory()
            clf,history = NN.trainModel(xTrain,yTrain,*validation_data,
                                     h,NEpochs=NEpochs,EarlyStopping=earlyStopping,
                                     nHidden=nHidden,optimizer=optimizer,
                                    batch_size=batch_size)
            losses[i] = history.history["val_loss"][-1]
            history.modelParams = {"h":h,"nHidden":nHidden,"earlyStopping":earlyStopping,
                                   "optimizer":optimizerName}
            models.append(clf)
            history.history["params"] = {"h": h,"nHidden": nHidden, "earlyStopping":earlyStopping,
                                         "optimizer": optimizerName}
            histories.append(history)
        self.models = models
        self.histories = histories
        self.modelOrder = np.argsort(losses)

    def predict(self,X,ensembleN=1):
        preds = np.zeros((ensembleN, X.shape[0]))
        for i,modelIdx in enumerate(self.modelOrder[:ensembleN]):
            preds[i] = self.models[modelIdx].predict(X,verbose=0).ravel()
        return preds.mean(0)

    def save(self,savepath):
        for modelNum,(model,history) in enumerate(zip(self.models,self.histories)):
            model.save(os.path.join(savepath,f"model_{modelNum}"))
            with open(os.path.join(savepath,f"history_{modelNum}.pkl"), 'wb') as file_pi:
                pickle.dump(history.history, file_pi)
        np.save(os.path.join(savepath,"modelOrder.npy"),self.modelOrder)

    @classmethod
    def from_filepath(cls,path):
        models = []
        histories = []
        nModels = len(glob(os.path.join(path,"model_*")))
        for modelNum in range(nModels):
            models.append(tf.keras.models.load_model(os.path.join(path,f"model_{modelNum}"),
                                                    compile=False))
            with open(os.path.join(path,f"history_{modelNum}.pkl"),"rb") as f:
                histories.append(History(pickle.load(f)))
        modelOrder = np.load(os.path.join(path,"modelOrder.npy"))
        obj = cls()
        obj.models = models
        obj.histories = histories
        obj.modelOrder = modelOrder
        return obj


# Cell
import xgboost as xgb

class XGBoost(object):
    def __init__(self,*args,**kwargs):
        super().__init__(*args,**kwargs)

    def fit(self,XTrain,yTrain,validation_data=None,**kwargs):
        dtrain = xgb.DMatrix(XTrain,yTrain)
        dval = xgb.DMatrix(*validation_data)
        self.clf = xgb.XGBClassifier(tree_method='gpu_hist')
        self.clf.fit(XTrain,yTrain,eval_set=[validation_data],)

    def predict(self,X):
        return self.clf.predict(X)

    def save(self,savepath):
        self.clf.save_model(savepath)

    @classmethod
    def from_filepath(cls,path):
        model = cls()
        model.clf = xgb.XGBClassifier()
        model.clf.load_model(path)
        return model


# Cell
from sklearn.isotonic import IsotonicRegression
import pickle
import joblib

class StepCalibrator:
    def __init__(self,mean=None):
        self.mean = mean

    def fit(self,X,y):
        self.mean = X.mean()

    def predict_proba(self,X):
        assert self.mean is not None
        probs = np.zeros((len(X),2))
        probs[:,1] = (X > self.mean).ravel()
        return probs

    def save(self,savepath,compresslevel=3):
        joblib.dump(self,savepath,compress=compresslevel)

    @classmethod
    def from_filepath(cls,path):
        return joblib.load(path)

# Cell

from sklearn.ensemble import RandomForestClassifier
import pickle
import joblib

class RandomForest(RandomForestClassifier):
    def __init__(self,params=None,n_estimators=500,
                 n_jobs=-1,max_depth=None,verbose=False):
        super().__init__(n_estimators=n_estimators,n_jobs=n_jobs,max_depth=max_depth,verbose=verbose)
        self.params=params
        if params is not None:
            print("setting params")
            self.set_params(**params)

    def save(self,savepath,compresslevel=3):
        joblib.dump(self,savepath,compress=compresslevel)

    def predict(self,X):
        return super().predict_proba(X)[:,1]

    @classmethod
    def from_filepath(cls,path):
        return joblib.load(path)

# Cell
from sklearn.linear_model import LogisticRegression
import pickle

class PlattCalibrator(LogisticRegression):
    def __init__(self,*args,**kwargs):
        super().__init__(*args,**kwargs)
        self.retVal = None

    def save(self,savepath):
        with open(savepath,"wb") as f:
            pickle.dump(self,f)

    def fit(self,scores,y):
        if np.all(y==1):
            self.retVal = 1
        elif np.all(y==0):
            self.retVal = 0
        else:
            super().fit(scores,y)

    def predict_proba(self,scores):
        if self.retVal is not None:
            r = np.ones((2,len(scores)))
            r[:,1] = self.retVal
            return r #return np.ones(len(scores)) * self.retVal
        return super().predict_proba(scores)

    @classmethod
    def from_filepath(cls,path):
        with open(path,"rb") as f:
            return pickle.load(f)

# Cell
def weighted_quantile(values, quantiles, sample_weight=None,
                      values_sorted=False, old_style=False):
    """ Very close to numpy.percentile, but supports weights.
    NOTE: quantiles should be in [0, 1]!
    :param values: numpy.array with data
    :param quantiles: array-like with many quantiles needed
    :param sample_weight: array-like of the same length as `array`
    :param values_sorted: bool, if True, then will avoid sorting of
        initial array
    :param old_style: if True, will correct output to be consistent
        with numpy.percentile.
    :return: numpy.array with computed quantiles.

    source: https://stackoverflow.com/questions/21844024/weighted-percentile-using-numpy
    """
    values = np.array(values)
    quantiles = np.array(quantiles)
    if sample_weight is None:
        sample_weight = np.ones(len(values))
    sample_weight = np.array(sample_weight)
    assert np.all(quantiles >= 0) and np.all(quantiles <= 1), \
        'quantiles should be in [0, 1]'

    if not values_sorted:
        sorter = np.argsort(values)
        values = values[sorter]
        sample_weight = sample_weight[sorter]

    weighted_quantiles = np.cumsum(sample_weight) - 0.5 * sample_weight
    if old_style:
        # To be convenient with numpy.percentile
        weighted_quantiles -= weighted_quantiles[0]
        weighted_quantiles /= weighted_quantiles[-1]
    else:
        weighted_quantiles /= np.sum(sample_weight)
    return np.interp(quantiles, weighted_quantiles, values)

# Cell
import os
from sklearn.linear_model import LogisticRegression
import numpy as np
from sklearn.model_selection import train_test_split
from tqdm import tqdm,trange
import xgboost as xgb
from sklearn.cluster import MiniBatchKMeans,KMeans
from sklearn.metrics import silhouette_score
from sklearn.metrics import roc_auc_score
import shutil
import pickle

class Method:
    def __init__(self,savepath=None,**kwargs):
        self.savepath = savepath
        if savepath is not None:
            if os.path.isdir(savepath):
                shutil.rmtree(savepath)
            os.mkdir(savepath)
        self.ensembleN = kwargs.get("ensembleN",1)

    def estimateClustering(self,X,**kwargs):
        silhouette_bootstrap_size = kwargs.get("silhouette_bootstrap_size",4096)
        batch_size= kwargs.get("batch_size",2^8)
        verbose = kwargs.get("verbose",True)
        reassignment_ratio = kwargs.get("reassignment_ratio",.001)
        tol = kwargs.get("tol",.01)
        scores = []
        if not len(kwargs.get("cluster_range")):
            raise ValueError("Cluster Range is empty")
        for k in tqdm(kwargs.get("cluster_range")):
            if k == 1:
                scores.append(0)
                continue
            if kwargs.get("minibatchKMeans",True):
                kmeans = MiniBatchKMeans(n_clusters=k,verbose=verbose,
                                         compute_labels=False,
                                         batch_size=batch_size,
                                         reassignment_ratio=reassignment_ratio,
                                         tol=tol)
            else:
                kmeans = KMeans(n_clusters=k)
            kmeans.fit(X)
            valindices = np.random.randint(0,len(X),size=silhouette_bootstrap_size)
            scores.append(silhouette_score(X[valindices],kmeans.predict(X[valindices])))
        print(scores)
        K = kwargs.get("cluster_range")[np.argmax(scores)]
        print(f"found {K} clusters...")
        self.clusterer = MiniBatchKMeans(n_clusters=K,verbose=verbose,
                                         compute_labels=False,
                                         batch_size=batch_size,
                                         reassignment_ratio=reassignment_ratio,
                                         tol=tol)
        self.clusterer.fit(X)

    def savemodels(self):
        if self.savepath is None:
            return
        np.save(os.path.join(self.savepath,"valClusterPriors.npy"),self.valClusterPriors)
        for k,(clf,calib) in enumerate(zip(self.classifiers, self.calibrators)):
            clf.save(os.path.join(self.savepath,f"classifier_cluster_{k}"))
            calib.save(os.path.join(self.savepath,f"calibrator_cluster_{k}"))

    def learnClusterModels(self,XTrain,yTrain,XVal,yVal,**kwargs):
        self.useXG = kwargs.get("useXG",False)
        self.useNN = kwargs.get("useNN",False)
        self.useRF = kwargs.get("useRF",True)

        trainClusters = self.clusterer.predict(XTrain)
        valClusters = self.clusterer.predict( XVal)
        self.classifiers = []
        self.calibrators = []
        self.valClusterPriors = []
        self.skipLabelShift = np.zeros(self.clusterer.n_clusters,dtype=bool)
        for k in trange(self.clusterer.n_clusters):
            trainMask = trainClusters == k
            valMask = valClusters == k
            if not trainMask.sum() or not valMask.sum():
                trainMask = np.ones(len(trainClusters)).astype(bool)
                valMask = np.ones(len(valClusters)).astype(bool)
            XTrainK,yTrainK = XTrain[trainMask], yTrain[trainMask]
            XValK,yValK = XVal[valMask], yVal[valMask]
            nnkwargs = kwargs.get("nnKwargs",{})
            xgkwargs = kwargs.get("xgKwargs",{})
            rfkwargs = kwargs.get("rfKwargs",{"n_estimators":500,
                                              "n_jobs":-1,
                                              "max_depth":7})
            if self.useNN:
                clf = NN()
            elif self.useXG:
                clf = XGBoost()
            else:
                clf = RandomForest(**rfkwargs)
            if self.useNN:
                clf.fit(XTrainK,yTrainK,validation_data=(XValK,yValK),**nnkwargs)
            elif self.useRF:
                clf.fit(XTrainK,yTrainK)
            else:
                clf.fit(XTrainK,yTrainK,validation_data=(XValK,yValK),params=xgkwargs)
            platt = PlattCalibrator()
            if self.useNN:
                valPreds = clf.predict(XValK,ensembleN=self.ensembleN)[:,None]
            else:
                valPreds = clf.predict(XValK)[:,None]
            platt.fit(valPreds,yValK)
            self.classifiers.append(clf)
            self.calibrators.append(platt)
            if len(np.unique(yValK)) == 2:
                valAUCk = roc_auc_score(yValK,valPreds)
                print(f"Cluster {k} validation auc: {valAUCk:.3f}")
            self.valClusterPriors.append(self.calibrators[-1].predict_proba(valPreds)[:,1].mean())
        self.savemodels()

    @staticmethod
    def rescalePosterior(alphaPrime,alpha,posterior):
        if alphaPrime == 1:
            return np.ones_like(posterior)
        elif alphaPrime == 0:
            return np.zeros_like(posterior)
        s1 = (alphaPrime / alpha) * posterior
        s0 = ((1-alphaPrime)/(1-alpha)) * (1-posterior)
        return s1/(s1+s0)

    def _getBagPriorEstimates(self,XUTrain,bagUTrain,**kwargs):
        self.priorEstimates = np.zeros((np.max(bagUTrain)+1, self.clusterer.n_clusters))
        self.emCurves = np.zeros(self.priorEstimates.shape+(kwargs.get("emIters",100)+1,))
        self.isOccupied = np.zeros_like(self.priorEstimates).astype(bool)
        self.clusterPreds = self.clusterer.predict(XUTrain)
        self.posts = np.zeros(len(bagUTrain))
        for k in trange(self.clusterer.n_clusters):
            clusterMask = self.clusterPreds == k
            assert clusterMask.any(), f"Cluster {k} appears to be empty"
            Xk = XUTrain[clusterMask]
            if self.useNN:
                scoresK = self.classifiers[k].predict(Xk,ensembleN=self.ensembleN)
            else:
                scoresK = self.classifiers[k].predict(Xk)
            self.posts[clusterMask] = self.calibrators[k].predict_proba(scoresK[:,None])[:,1]

        for b in np.unique(bagUTrain):
            bagMask = bagUTrain == b
            bagClusterPreds = self.clusterPreds[bagMask]
            for k in range(self.clusterer.n_clusters):
                clusterMask = bagClusterPreds == k
                postsBK = self.posts[bagMask][clusterMask]
                if not len(postsBK):
                    continue
                self.isOccupied[b,k] = True
                alphaPrime = postsBK.mean()
                self.emCurves[b,k,0] = alphaPrime
                for iteration in range(kwargs.get("emIters",100)):
                    alphaPrime = Method.rescalePosterior(alphaPrime,self.valClusterPriors[k],postsBK).mean()
                    self.emCurves[b,k,iteration+1] = alphaPrime
                self.priorEstimates[b,k] = alphaPrime
        self.savePriorEsts()

    def savePriorEsts(self):
        if self.savepath is None:
            return
        np.save(os.path.join(self.savepath,"priorEstimates.npy"),self.priorEstimates)
        np.save(os.path.join(self.savepath,"emCurves.npy"),self.emCurves)
        np.save(os.path.join(self.savepath,"isOccupied.npy"),self.isOccupied)
        np.save(os.path.join(self.savepath,"clusterPreds.npy"),self.clusterPreds)
        np.save(os.path.join(self.savepath,"posts.npy"),self.posts)

    def estimatePriors(self,XUTrain,bagUTrain,**kwargs):
        self._getBagPriorEstimates(XUTrain,bagUTrain,**kwargs)

    def predict(self,X,bag=None,clusterGlobal=False,bagLocal=False):
        assert (clusterGlobal + bagLocal) <= 1, "cannot specify clusterGlobal and bagLocal"
        if len(X.shape) == 1 and type(bag) is int:
            # cast a single test instance as a set of samples with size 1
            X = X[None]
            bag = np.array([bag])
        clusterPreds = self.clusterer.predict(X)
        preds = np.zeros(len(X))
        for k in np.unique(clusterPreds):
            clusterMask = clusterPreds == k
            if self.useNN:
                scores = self.classifiers[k].predict(X[clusterMask],ensembleN=self.ensembleN)
            else:
                scores = self.classifiers[k].predict(X[clusterMask])
            posts = self.calibrators[k].predict_proba(scores[:,None])[:,1]
            if clusterGlobal:
                preds[clusterMask] = posts
            else:
                for b in np.unique(bag):
                    bagMask = bag[clusterMask] == b
                    alphaPrime = self.priorEstimates[b,k]
                    preds[clusterMask & (bag == b)] = Method.rescalePosterior(alphaPrime,
                                                               self.valClusterPriors[k],
                                                              posts[bagMask])
        return preds

    def fit(self,XTrain,yTrain,XVal,yVal,XUTrain,bagUTrain,**kwargs):
        estimatedClustering = kwargs.get("estimatedClustering",None)
        if estimatedClustering is None:
            self.estimateClustering(np.concatenate((XTrain,XVal,XUTrain)),**kwargs)
        else:
            print("skipping clustering...")
            self.clusterer = estimatedClustering
        try:
            with open(os.path.join(self.savepath,"clusterer.pkl"),"wb") as f:
                pickle.dump(self.clusterer,f)
        except AttributeError:
            print("cannot pickle clusterer, probably is the true clusterer of synthetic data")
        self.learnClusterModels(XTrain,yTrain,XVal,yVal,**kwargs)
        self.estimatePriors(XUTrain,bagUTrain,**kwargs)

# Cell
from tqdm import tqdm,trange
from sklearn.ensemble import RandomForestClassifier

class LocalMethod:
    def __init__(self,savepath=None,**kwargs):
        self.savepath = savepath
        if savepath is not None:
            if os.path.isdir(savepath):
                shutil.rmtree(savepath)
            os.mkdir(savepath)

    def learnModels(self,XLabeledTrain,yLabeledTrain,bagLabeledTrain,XLabeledVal,yLabeledVal,bagLabeledVal,*args,**kwargs):
        useNN = kwargs.get("useNN",False)
        useXG = kwargs.get("useXG",False)
        useRF = kwargs.get("useRF",True)
        self.classifiers = []
        self.calibrators = []
        self.valPriors = np.ones(max(bagLabeledTrain.max(),
                                     bagLabeledVal.max())+1)
        for b in trange(len(self.valPriors)):
            maskTrain = bagLabeledTrain == b
            maskVal = bagLabeledVal == b
            if not maskTrain.sum():
                classifiers.append(None)
                calibrators.append(None)
                continue
            XbTrain = XLabeledTrain[maskTrain]
            ybTrain = yLabeledTrain[maskTrain]
            XbVal = XLabeledVal[maskVal]
            ybVal = yLabeledVal[maskVal]
            nnkwargs = kwargs.get("nnKwargs",{})
            xgkwargs = kwargs.get("xgKwargs",{})
            rfkwargs = kwargs.get("rfKwargs",{"n_estimators":500,
                                              "n_jobs":-1,
                                             "max_depth":7})
            if useNN:
                clf = NN()
            elif useXG:
                clf = XGBoost()
            else:
                clf = RandomForest(**rfkwargs)
            if useNN:
                clf.fit(XbTrain,ybTrain,validation_data=(XbVal,ybVal),**nnkwargs)
            elif useXG:
                clf.fit(XbTrain,ybTrain,validation_data=(XbVal,ybVal),params=xgkwargs)
            else:
                clf.fit(XbTrain,ybTrain)
            self.classifiers.append(clf)
            platt = PlattCalibrator()
            valPreds = clf.predict(XbVal)
            platt.fit(valPreds[:,None],ybVal)
            self.calibrators.append(platt)
            self.valPriors[b] = platt.predict_proba(valPreds[:,None])[:,1].mean()

    @staticmethod
    def rescalePosterior(alphaPrime,alpha,posterior):
        if alphaPrime == 1:
            return np.ones_like(posterior)
        elif alphaPrime == 0:
            return np.zeros_like(posterior)
        s1 = (alphaPrime / alpha) * posterior
        s0 = ((1-alphaPrime)/(1-alpha)) * (1-posterior)
        rescaled = s1 / (s1 + s0)
        return rescaled

    def labelShift(self,XUnlabeledTrain,bagUnlabeledTrain,*args,**kwargs):
        self.priorEstimates = np.zeros_like(self.valPriors)
        self.emCurves = np.zeros((self.priorEstimates.shape + (kwargs.get("emIters",100)+1,)))
        self.isOccupied = np.zeros_like(self.priorEstimates).astype(bool)
        for b in trange(len(self.priorEstimates)):
            bagMask = bagUnlabeledTrain == b
            Xb = XUnlabeledTrain[bagMask]
            scores = self.classifiers[b].predict(Xb)
            posts = self.calibrators[b].predict_proba(scores[:,None])[:,1]
            if not len(posts):
                continue
            self.isOccupied[b] = True
            alphaPrime = posts.mean()
            self.emCurves[b,0] = alphaPrime
            for iteration in trange(kwargs.get("emIters",100)):
                alphaPrime = LocalMethod.rescalePosterior(alphaPrime,self.valPriors[b],posts).mean()
            self.priorEstimates[b] = alphaPrime

    def saveLabelShift(self):
        np.save(os.path.join(self.savepath,"priorEstimates.npy"),self.priorEstimates)
        np.save(os.path.join(self.savepath,"emCurves.npy"),self.emCurves)
        np.save(os.path.join(self.savepath,"isOccupied.npy"),self.isOccupied)

    def savemodels(self):
        if self.savepath is None:
            return
        np.save(os.path.join(self.savepath,"valPriors.npy"),self.valPriors)
        for b,(clf,calib) in enumerate(zip(self.classifiers, self.calibrators)):
            clf.save(os.path.join(self.savepath,f"classifier_bag_{b}"))
            calib.save(os.path.join(self.savepath,f"calibrator_bag_{b}"))

    def fit(self,XTrain,yTrain,bagLabeledTrain,XVal,yVal,bagLabeledVal,XUnlabeledTrain,bagUnlabeledTrain,**kwargs):
        self.learnModels(XTrain,yTrain,bagLabeledTrain,XVal,yVal,bagLabeledVal,**kwargs)
        self.savemodels()
        self.labelShift(XUnlabeledTrain,bagUnlabeledTrain,**kwargs)
        self.saveLabelShift()

    def predict(self,X,bags,labelShift=False):
        posts = np.zeros(len(X))
        for b in np.unique(bags):
            bagmask = bags==b
            Xb = X[bagmask]
            scores = self.classifiers[b].predict(Xb)
            p = self.calibrators[b].predict_proba(scores[:,None])[:,1]
            if labelShift:
                posts[bagmask] = LocalMethod.rescalePosterior(self.priorEstimates[b],self.valPriors[b],p)
            else:
                posts[bagmask] = p
        return posts

# Cell
class GroupAwareGlobal:
    def __init__(self,savepath=None,**kwargs):
        self.savepath = savepath
        if savepath is not None:
            if os.path.isdir(savepath):
                shutil.rmtree(savepath)
            os.mkdir(savepath)

    def learnModels(self,XLabeledTrain,yLabeledTrain,bagLabeledTrain,XLabeledVal,yLabeledVal,bagLabeledVal,*args,**kwargs):
        useNN = kwargs.get("useNN",False)
        useXG = kwargs.get("useXG",False)
        useRF = kwargs.get("useRF",True)
        self.nbags = bagLabeledTrain.max()+1
        print("creating matrices")
        bagOneHotTrain = np.zeros((len(XLabeledTrain),self.nbags))
        bagOneHotTrain[np.arange(len(bagLabeledTrain)),bagLabeledTrain] = 1
        bagOneHotVal = np.zeros((len(XLabeledVal),self.nbags))
        bagOneHotVal[np.arange(len(bagLabeledVal)),bagLabeledVal] = 1
        XT = np.concatenate((XLabeledTrain,bagOneHotTrain),axis=1)
        XV = np.concatenate((XLabeledVal,bagOneHotVal),axis=1)
        print("done")
        nnkwargs = kwargs.get("nnKwargs",{})
        xgkwargs = kwargs.get("xgKwargs",{})
        rfkwargs = kwargs.get("rfKwargs",{"n_estimators":500,
                                          "n_jobs":-1,
                                         "verbose":True,
                                         "max_depth": 7})
        if useNN:
            clf = NN()
        elif useXG:
            clf = XGBoost()
        else:
            clf = RandomForest(**rfkwargs)
        print("training")
        if useNN:
            clf.fit(XT,yLabeledTrain,validation_data=(XV,yLabeledVal),**nnkwargs)
        elif useXG:
            clf.fit(XT,yLabeledTrain,validation_data=(XV,yLabeledVal),params=xgkwargs)
        else:
            clf.fit(XT,yLabeledTrain)
        self.classifier = clf
        platt = PlattCalibrator()
        valPreds = clf.predict(XV)
        platt.fit(valPreds[:,None],yLabeledVal)
        self.calibrator = platt

    def savemodels(self):
        if self.savepath is None:
            return
        self.classifier.save(os.path.join(self.savepath,f"classifier"))
        self.calibrator.save(os.path.join(self.savepath,"calibrator"))

    def fit(self,XTrain,yTrain,bagLabeledTrain,XVal,yVal,bagLabeledVal,XUnlabeledTrain,bagUnlabeledTrain,**kwargs):
        self.learnModels(XTrain,yTrain,bagLabeledTrain,XVal,yVal,bagLabeledVal,**kwargs)
        self.savemodels()

    def predict(self,X,bags):
        posts = np.zeros(len(X))
        bagOneHot = np.zeros((len(X),self.nbags))
        bagOneHot[np.arange(len(bags)),bags] = 1
        XAugmented = np.concatenate((X,bagOneHot),axis=1)
        scores = self.classifier.predict(XAugmented)
        p = self.calibrator.predict_proba(scores[:,None])[:,1]
        return p

import os
import shutil
import numpy as np
import pickle
import scipy
from sklearn.metrics import roc_auc_score

class FrustratinglyEasyDomainAdaptation:
    def __init__(self,savepath=None,**kwargs):
        self.savepath = savepath
        if savepath is not None:
            if os.path.isdir(savepath):
                shutil.rmtree(savepath)
            os.mkdir(savepath)

    def CORAL(self,Ds,Dt):
        Cs = np.cov(Ds.T) + np.eye(Ds.shape[1])
        Ct = np.cov(Dt.T) + np.eye(Dt.shape[1])
        Ds = Ds @ scipy.linalg.sqrtm(np.linalg.inv(Cs))
        DsStar = Ds @ scipy.linalg.sqrtm(Ct)
        return DsStar
    
    def fitModel(self,XTrain,yTrain,XVal,yVal,**kwargs):
        self.useRF = kwargs.get("useRF",True)
        if self.useRF:
            rfkwargs = kwargs.get("rfKwargs",{"n_estimators":500,
                                          "n_jobs":-1,
                                         "verbose":True,
                                         "max_depth": 7})
            clf = RandomForest(**rfkwargs)
            clf.fit(XTrain,yTrain)
        else:
            raise NotImplementedError()
        valPreds = clf.predict(XVal)[:,None]
        print("val auc: ",roc_auc_score(yVal,valPreds.ravel()))
        platt = PlattCalibrator()
        platt.fit(valPreds,yVal)
        self.classifiers.append(clf)
        self.calibrators.append(platt)

    def fit(self,XTrain,yTrain,XVal,yVal,XUTrain,bagUTrain,**kwargs):
        self.classifiers = []
        self.calibrators = []
        self.domain2Idx = {}
        Xs = np.concatenate((XTrain,XVal))
        ntrain = XTrain.shape[0]
        for i,targetDomain in enumerate(np.unique(bagUTrain)):
            self.domain2Idx[targetDomain] = i
            domainMask = bagUTrain == targetDomain
            Xt = XUTrain[domainMask]
            DsStar = self.CORAL(Xs,Xt)
            DStarTrain, DStarVal = DsStar[:ntrain],DsStar[ntrain:]
            self.fitModel(DStarTrain,yTrain,DStarVal,yVal,**kwargs)
        self.savemodels()
    
    def savemodels(self):
        if self.savepath is None:
            return
        with open(os.path.join(self.savepath,"domain2Idx.pkl"),"wb") as f:
            pickle.dump(self.domain2Idx,f)
        for i, (clf,platt) in enumerate(zip(self.classifiers,self.calibrators)):
            clf.save(os.path.join(self.savepath,f"classifier_domain_{i}"))
            platt.save(os.path.join(self.savepath,f"calibrator_domain_{i}"))

    def predict(self,X,bags):
        indices = np.zeros(len(bags)).astype(int)
        for i,b in enumerate(bags):
            indices[i] = self.domain2Idx[b]
        preds = np.zeros(len(bags))
        for idx in np.unique(indices):
            idxMask = indices == idx
            Xidx = X[idxMask]
            scores = self.classifiers[idx].predict(Xidx)
            posts = self.calibrators[idx].predict_proba(scores[:,None])[:,1]
            preds[idxMask] = posts
        return preds
