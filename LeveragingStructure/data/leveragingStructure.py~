# AUTOGENERATED! DO NOT EDIT! File to edit: 03_ACS_Data.ipynb (unless otherwise specified).

__all__ = ['Pool', 'StructuredPool', 'RandomSplitter', 'Resampler', 'ParameterizedResampler', 'Partitioner',
           'DataLoader', 'RandomSplitLoader', 'Setting1Loader', 'Setting2Loader', 'getGroups', 'SyntheticPool',
           'SyntheticDatasetBase', 'SyntheticSetting1', 'SyntheticSetting2', 'ACSLoader', 'ACSLoaderSetting1',
           'ACSLoaderSetting2',"HuggingfaceDataset","HuggingfaceDatasetSetting1","HuggingfaceDatasetSetting2"]

# Cell
from sklearn.preprocessing import StandardScaler

class Pool(object):
    def __init__(self,x,y,*args,**kwargs):
        super(Pool,self).__init__(*args,**kwargs)
        assert len(x) == len(y)
        self.x = x
        self.y = y

    def __getitem__(self,idx):
        return self.x[idx],self.y[idx]

    def __len__(self):
        return self.x.shape[0]

    def __add__(self,b):
        return Pool(np.concatenate((self.x,b.x)),
                    np.concatenate((self.y,b.y)))
    @property
    def classPrior(self):
        return self.y.sum() / len(self.y)

    def normalize(self):
        scaler = StandardScaler()
        self.x = scaler.fit_transform(self.x)

# Cell

class StructuredPool(Pool):
    """
    Data pool in which each instance is associated with a structure group identifier (SGID)
    """
    def __init__(self,x,y,sgid,instanceNum,maxSGID,augmentFeatures=False,*args,**kwargs):
        self.sgid = sgid.astype(int)
        self.instanceNum = instanceNum
        self._augmentFeatures = augmentFeatures
        self.maxSGID = maxSGID
        assert len(sgid) == len(x)
        assert len(instanceNum) == len(x)
        super().__init__(x,y,*args,**kwargs)

    @property
    def augmentFeatures(self):
        return self._augmentFeatures

    @augmentFeatures.setter
    def augmentFeatures(self,augmentFeatures):
        self._augmentFeatures = bool(augmentFeatures)

    def __add__(self,b):
        return StructuredPool(np.concatenate((self.x,b.x)),
                              np.concatenate((self.y,b.y)),
                              np.concatenate((self.sgid,b.sgid)),
                              np.concatenate((self.instanceNum,b.instanceNum)),
                              max(self.maxSGID,b.maxSGID),
                              augmentFeatures=self.augmentFeatures or b.augmentFeatures)

    def _getx(self,idx):
        if self.augmentFeatures:
            x = self.x[idx]
            sgids = self.sgid[idx]
            structure = np.zeros((sgids.size, self.maxSGID+1))
            structure[np.arange(sgids.size),sgids] = 1
            return np.concatenate((x,structure),axis=1)
        else:
            return self.x[idx]

    def __getitem__(self,idx):
        x,y = self._getx(idx),self.y[idx]
        return x,y

    def getaugmentation(self,idx):
        sgids = self.sgid[idx]
        try:
            sgids[0]
        except TypeError:
            raise ValueError("idx must be a list of indices")
        onehot = np.zeros((len(sgids),self.maxSGID + 1))
        onehot[np.arange(len(sgids)),sgids] = 1
        return onehot

    def get(self,idx):
        return StructuredPool(self.x[idx],self.y[idx],
                              self.sgid[idx],
                              self.instanceNum[idx],
                              self.maxSGID,
                              augmentFeatures=self.augmentFeatures)

    def getgroup(self,groupNum):
        mask = self.sgid == groupNum
        return StructuredPool(self.x[mask],self.y[mask],
                              self.sgid[mask],
                              self.instanceNum[mask],
                              self.maxSGID,
                              augmentFeatures=self.augmentFeatures)

# Cell
from sklearn.model_selection import train_test_split

class RandomSplitter(object):
    def __init__(self,*args,**kwargs):
        super(RandomSplitter,self).__init__(*args,**kwargs)

    def split(self,pool,labelProportion):
        labeledIndices,unlabeledIndices = train_test_split(np.arange(len(pool)),train_size=labelProportion)
        labeledPool = pool.get(labeledIndices)
        unlabeledPool = pool.get(unlabeledIndices)
        return labeledPool,unlabeledPool

# Cell
import warnings

class Resampler(object):
    def __init__(self,classPrior,*args,**kwargs):
        super(Resampler,self).__init__(*args,**kwargs)
        self.classPrior = classPrior
    """
    Resample the given data pool such that the resulting pool has a given label proportion
    """
    @staticmethod
    def getClassIndices(pool):
        pos_indices = np.where(pool.y)[0]
        neg_indices = np.array(list(set(np.arange(len(pool))) - set(pos_indices))).astype(int)
        return pos_indices,neg_indices

    def downsample(self,pool):
        pos_indices,neg_indices = Resampler.getClassIndices(pool)
        posAcceptanceProb = len(neg_indices) * self.classPrior / len(pool)
        negAcceptanceProb = len(pos_indices) * (1 - self.classPrior) / len(pool)
        resampledPositiveIndices = pos_indices[np.random.binomial(1,posAcceptanceProb,size=len(pos_indices)).astype(bool)]
        resampledNegativeIndices = neg_indices[np.random.binomial(1,negAcceptanceProb,size=len(neg_indices)).astype(bool)]
        resampledIndices = np.concatenate((resampledPositiveIndices,resampledNegativeIndices))
        return pool.get(resampledIndices)

    def upsample(self,pool,targetN):
        """
        Resample positive and negative instances (with replacement) to ensure label proportion
        """
        pos_indices,neg_indices = Resampler.getClassIndices(pool)
        desiredNumPositive = int(np.round(targetN * self.classPrior))
        desiredNumNegative = targetN - desiredNumPositive
        if not len(pos_indices) and desiredNumPositive:
            warnings.warn("There are no positive instances to sample from")
            desiredNumPositive = 0
        if not len(neg_indices) and desiredNumNegative:
            warnings.warn("There are no negative instanes to sample from")
            desiredNumNegative = 0
        if not desiredNumPositive and desiredNumNegative and desiredNumNegative < targetN:
            desiredNumNegative = targetN
        if not desiredNumNegative and desiredNumPositive and desiredNumPositive < targetN:
            desiredNumPositive = targetN

        resampledPositiveIndices = np.random.choice(pos_indices,
                                                    size=desiredNumPositive,
                                                    replace=True)
        resampledNegativeIndices = np.random.choice(neg_indices,
                                                    size=desiredNumNegative,
                                                    replace=True)
        resampledIndices = np.concatenate((resampledPositiveIndices,resampledNegativeIndices))
        upsampledPool = pool.get(resampledIndices)
        if len(np.unique(upsampledPool.instanceNum)) < 5:
            warnings.warn("Upsampling resulted in a pool with fewer than 5 unique instances; This could cause issues with generating cross validation pools")
        return upsampledPool

    def resample(self,pool,downsample=True,**kwargs):
        if downsample:
            return self.downsample(pool)
        return self.upsample(pool,len(pool))

# Cell
from sklearn.cluster import KMeans,MiniBatchKMeans
import warnings
from sklearn.random_projection import GaussianRandomProjection

class ParameterizedResampler(object):
    """
    Cluster the data pool then resample each cluster with given parameters
    """
    def __init__(self,alpha=None,pi=None,rho=None,eta=None,gamma=None,clusterer=None,*args,**kwargs):
        super(ParameterizedResampler,self).__init__(*args,**kwargs)

        self.alpha = alpha
        self.pi = pi
        self.rho = rho
        if alpha is not None and pi is not None and rho is not None and eta is None and gamma is None:
            assert len(pi) == len(rho), "Need to specify same number of positive and negative components"
            self.gamma = alpha * pi + (1 - alpha) * rho
            self.eta = alpha * pi / self.gamma
            self.n_clusters = len(pi)
        elif eta is not None and gamma is not None and alpha is None and pi is None and rho is None:
            self.gamma = gamma
            self.eta = eta
            self.n_clusters = len(eta)
        else:
            raise ValueError("Invalid set of keyword arguments; either specify alpha,pi, and rho OR eta and gamma")
        self.clusterer = clusterer

    def fit(self,X,minibatchKMeans=True):
        if minibatchKMeans:
            print("using minibatch")
            kmeans = MiniBatchKMeans(n_clusters=self.n_clusters,verbose=1,compute_labels=False,batch_size=2^13)
        else:
            kmeans = KMeans(n_clusters=self.n_clusters)
        kmeans.fit(X)
        self.clusterer = kmeans

    @staticmethod
    def getClassIndices(pool):
        pos_indices = np.where(pool.y)[0]
        neg_indices = np.array(list(set(np.arange(len(pool))) - set(pos_indices))).astype(int)
        return pos_indices,neg_indices

    @staticmethod
    def resamplePool(pool,desiredNumPositive,desiredNumNegative,**kwargs):
        allowDuplicates = kwargs.get("allowDuplicates",False)
        pos_indices,neg_indices = ParameterizedResampler.getClassIndices(pool)
        if allowDuplicates:
            N = desiredNumPositive + desiredNumNegative
            if not len(pos_indices) and desiredNumPositive:
                warnings.warn("No positive instances to sample from")
                num_pos = 0
            else:
                num_pos = desiredNumPositive
            if not len(neg_indices) and desiredNumNegative:
                warnings.warn("No negative instances to sample from")
                num_neg = 0
            else:
                num_neg = desiredNumNegative
            if not num_neg and num_pos and num_pos < N:
                num_pos = N
            if not num_pos and num_neg and num_neg < N:
                num_neg = N
            resampledPositiveIndices = np.random.choice(pos_indices,
                                                        size=num_pos,
                                                        replace=True)
            resampledNegativeIndices = np.random.choice(neg_indices,
                                                        size=num_neg,
                                                        replace=True)
        else:
            resampledPositiveIndices = np.random.choice(pos_indices,
                                                       size=min(len(pos_indices), desiredNumPositive),
                                                       replace=False)
            resampledNegativeIndices = np.random.choice(neg_indices,
                                                       size=min(len(neg_indices), desiredNumNegative),
                                                       replace=False)
        resampledIndices = np.concatenate((resampledPositiveIndices,resampledNegativeIndices))
        upsampledPool = pool.get(resampledIndices)
        return upsampledPool

    def resample(self,pool,targetSize,**kwargs):
        if self.clusterer is None:
            raise ValueError("Need to first call fit to fit clustering model to the data set")
        x = pool.x
        clusterAssignments = self.clusterer.predict(x)
        pools = []
        for k,(gamma_k, eta_k) in enumerate(zip(self.gamma,self.eta)):
            desiredSize = targetSize * gamma_k
            desiredNumPositive = round(desiredSize * eta_k)
            desiredNumNegative = round(desiredSize * (1 - eta_k))
            pool_k = pool.get(clusterAssignments == k)
            if len(pool_k):
                pools.append(ParameterizedResampler.resamplePool(pool_k,desiredNumPositive, desiredNumNegative,**kwargs))


        if len(pools) == 1:
            return pools[0]
        return sum(pools[1:],pools[0])

# Cell
class Partitioner(object):
    def __init__(self,*args,**kwargs):
        super(Partitioner,self).__init__(*args,**kwargs)
    """
    Greedily merge structured groups into partitions of given minimum size
    """
    def partition(self,pool,minsize):
        singleGroupPartitions = [pool.getgroup(g) for g in np.unique(pool.sgid)]
        singleGroupPartitions = sorted(singleGroupPartitions,key=lambda pool:len(pool),reverse=True)
        groupedPartitions = [singleGroupPartitions.pop(0)]
        while len(singleGroupPartitions):
            g = singleGroupPartitions.pop(0)
            if len(groupedPartitions[-1]) < minsize:
                groupedPartitions.append(groupedPartitions.pop(-1) + g)
            else:
                groupedPartitions.append(g)
        if len(groupedPartitions[-1]) < minsize and len(groupedPartitions) >= 2:
            groupedPartitions.append(groupedPartitions.pop(-1)+groupedPartitions.pop(-1))
        return groupedPartitions

# Cell
from abc import ABC,abstractmethod
from warnings import warn
class DataLoader(ABC):
    def __init__(self,*args,**kwargs):
        super(DataLoader,self).__init__()
        warn("making structured pool")
        self.makeStructuredDataPool(*args,**kwargs)
        self.structuredPool.normalize()
        if kwargs.get('resampleGroupID',True):
            warn("Resampling group IDs")
            self.updateGroupIDs(*args,**kwargs)
        warn("partitioning dataset")
        self.partitionDataset(*args,**kwargs)
        warn("splitting partitions")
        self.splitPartitions(*args,**kwargs)
        warn("resampling partitions")
        self.resamplePools(*args,**kwargs)

    @property
    def augmentFeatures(self):
        return self._augmentFeatures

    @augmentFeatures.setter
    def augmentFeatures(self,augmentFeatures):
        self._augmentFeatures = bool(augmentFeatures)
        for pNum in range(len(self.unlabeledSamples)):
            self.unlabeledSamples[pNum].augmentFeatures = augmentFeatures
        for pNum in range(len(self.labeledSamples)):
            self.labeledSamples[pNum].augmentFeatures = augmentFeatures

    @abstractmethod
    def makeStructuredDataPool(self,*args,**kwargs):
        """
        load data and assign the class attribute self.structuredPool
        """
        self.structuredPool = StructuredPool(np.zeros(0),
                                             np.zeros(0).astype(bool),
                                             np.zeros(0),
                                             0)

    def updateGroupIDs(self,*args,**kwargs):
        groupNums,groupCount = np.unique(self.structuredPool.sgid,return_counts=True)
        resampledSGIDs = np.random.choice(groupNums,size=len(self.structuredPool),p=groupCount/len(self.structuredPool))
        self.structuredPool.sgid = resampledSGIDs
        self.structuredPool.maxSGID = np.max(resampledSGIDs)

    def makePartitioner(self,*args,**kwargs):
        """
        Return the partitioner object that will be used to partition the dataset into groups of bags
        """
        return Partitioner()


    def partitionDataset(self,*args,**kwargs):
        """
        Partition dataset into groups of bags each containing at lease 'minsize' instances
        """
        assert "minsize" in kwargs, "Need to specify kwarg minsize"
        self.partitioner = self.makePartitioner(*args,**kwargs)
        self.partitions = self.partitioner.partition(self.structuredPool,
                                                              kwargs["minsize"])
        del self.structuredPool
        partitionSizes = np.array([len(p) for p in self.partitions])
        assert np.all(partitionSizes >= kwargs["minsize"]),"Error in partitioner: at least one partition is too small; partitionSizes: {}".format(partitionSizes)

    @abstractmethod
    def makeSplitter(self,*args,**kwargs):
        """
        Return the splitter object that will be used to split each partition into disjoint labeled and unlabeled pools
        """
        pass

    def splitPartitions(self,*args,**kwargs):
        """
        split each partition into a labeled and unlabeled pool
        """
        self.splitter = self.makeSplitter()
        self.labeledPools = [None for _ in range(len(self.partitions))]
        self.unlabeledPools = [None for _ in range(len(self.partitions))]
        for j,partition_j in enumerate(self.partitions):
            self.labeledPools[j],self.unlabeledPools[j] = self.splitter.split(partition_j,
                                                                                  kwargs["labelProportion"])
        del self.partitions
    @abstractmethod
    def resamplePools(self,*args,**kwargs):
        """Resample each labeled and unlabeled pool based on the experiment setting"""
        pass

# Cell
class RandomSplitLoader(object):
    def __init__(self,*args,**kwargs):
        super(RandomSplitLoader,self).__init__(*args,**kwargs)
    def makeSplitter(self,*args,**kwargs):
        return RandomSplitter()

# Cell
import numpy as np
from sklearn.cluster import KMeans

class Setting1Loader(object):
    def __init__(self,*args,**kwargs):
        super(Setting1Loader,self).__init__(*args,**kwargs)

    def resamplePools(self,*args,failCount=0,**kwargs):
        gamma = np.random.dirichlet(np.ones(1) * 2)
        eta = np.random.uniform(0.01,0.99,size=1)
        self.resampler = ParameterizedResampler(eta=eta,gamma=gamma)
        XFull = np.concatenate([p[:][0] for p in self.labeledPools + self.unlabeledPools])
        self.bootstrappedSample = XFull[np.random.randint(0,len(XFull),size=len(XFull))]
        self.resampler.fit(XFull,minibatchKMeans=kwargs.get("minibatchKMeans",True))
        self.labeledSamples = [None for _ in range(len(self.labeledPools))]
        self.unlabeledSamples = [None for _ in range(len(self.unlabeledPools))]
        for p,(lp,up) in enumerate(zip(self.labeledPools, self.unlabeledPools)):
            self.labeledSamples[p] = self.resampler.resample(lp,kwargs["bagLabeledSampleDistribution"](len(lp)))
            self.unlabeledSamples[p] = self.resampler.resample(up,kwargs["bagUnlabeledSampleDistribution"](len(up)))
        self.trueClusterer = self.resampler.clusterer

# Cell
import numpy as np
from sklearn.cluster import KMeans
from tqdm import tqdm
from sklearn.metrics import silhouette_score
from itertools import repeat
from multiprocessing import Pool as PPool

class Setting2Loader(object):
    def __init__(self,*args,**kwargs):
        super(Setting2Loader,self).__init__(*args,**kwargs)

    def getSilhouetteScore(self,X,k,**kwargs):
        minibatchKMeans = kwargs.get("minibatchKMeans",True)
        silhouette_bootstrap_size = kwargs.get("silhouette_bootstrap_size",4096)
        batch_size= kwargs.get("batch_size",2^12)
        verbose = kwargs.get("verbose",True)
        reassignment_ratio = kwargs.get("reassignment_ratio",.001)
        tol = kwargs.get("tol",.01)
        if k == 1:
            return 0
        if minibatchKMeans:
            print("using minibatch")
            kmeans = MiniBatchKMeans(n_clusters=k,verbose=verbose,compute_labels=False,batch_size=batch_size,
                                    reassignment_ratio=reassignment_ratio,tol=tol)
        else:
            kmeans = KMeans(n_clusters=k)
        kmeans.fit(X)
        print("predicting...")
        if minibatchKMeans:
            XP = X[np.random.randint(0,len(X),size=silhouette_bootstrap_size)]
        else:
            XP = X
        return silhouette_score(XP,kmeans.predict(XP))

    def estimateClustering(self,X,**kwargs):
        cluster_range=kwargs.get("cluster_range")
        minibatchKMeans = kwargs.get("minibatchKMeans",True)
        silhouette_bootstrap_size = kwargs.get("silhouette_bootstrap_size",4096)
        batch_size= kwargs.get("batch_size",2^12)
        verbose = kwargs.get("verbose",True)
        tol = kwargs.get("tol",.01)
        reassignment_ratio = kwargs.get("reassignment_ratio",.001)
        """Use all the observed data to estimate the clustering"""
        scores = np.zeros(len(cluster_range))
        for i,k in tqdm(enumerate(cluster_range),total=len(cluster_range)):
            scores[i] = self.getSilhouetteScore(X,k,**kwargs)
        K = cluster_range[np.argmax(scores)]
        print("silhouette scores",cluster_range,"\n",scores)
        if minibatchKMeans:
            print("using minibatch for final clustering")
            bestClusterer = MiniBatchKMeans(n_clusters=K,verbose=verbose,
                                            compute_labels=False,
                                            batch_size=batch_size,
                                            tol=tol,
                                           reassignment_ratio=reassignment_ratio)
        else:
            bestClusterer = KMeans(n_clusters=K)
        bestClusterer.fit(X)
        return bestClusterer

    def resamplePools(self,*args,failCount=0,**kwargs,):
        self.labeledResamplers = []
        self.unlabeledResamplers = []
        X = np.concatenate([p[:][0] for p in self.labeledPools + self.unlabeledPools])
        y = np.concatenate([p[:][1] for p in self.labeledPools + self.unlabeledPools])
        kmeans = self.estimateClustering(X,**kwargs)
        print("Found {} clusters".format(kmeans.n_clusters))
        clusterPreds = kmeans.predict(X)
        num_pos = np.zeros(kmeans.n_clusters)
        num_neg = np.zeros_like(num_pos)
        for k in range(num_pos.shape[0]):
            num_pos[k] = y[clusterPreds == k].sum()
            num_neg[k] = (clusterPreds == k).sum() - num_pos[k]
        self.trueClusterer = kmeans
        self.labeledSamples = [None for _ in range(len(self.labeledPools))]
        self.unlabeledSamples = [None for _ in range(len(self.unlabeledPools))]
        for p,(lp,up) in tqdm(enumerate(zip(self.labeledPools, self.unlabeledPools)),
                              total=len(self.labeledPools)):
            gamma = np.random.dirichlet(np.ones(kmeans.n_clusters) * 2,size=2)
            eta = np.random.uniform(0.01,0.99,size=(2,kmeans.n_clusters))
            NL = kwargs["bagLabeledSampleDistribution"](len(lp))
            NU = kwargs["bagUnlabeledSampleDistribution"](len(up))
            xl,yl = lp[:]
            lk = kmeans.predict(xl)
            NP = np.array([((lk == k) & yl).sum() for k in range(kmeans.n_clusters)])
            NN = np.array([((lk == k) & ~yl).sum() for k in range(kmeans.n_clusters)])
            labeledResampler = ParameterizedResampler(eta=eta[0],gamma=gamma[0],clusterer=kmeans)
            unlabeledResampler = ParameterizedResampler(eta=eta[1],gamma=gamma[1],clusterer=kmeans)
            self.labeledSamples[p] = labeledResampler.resample(lp,NL,**kwargs)
            self.unlabeledSamples[p] = unlabeledResampler.resample(up,NU,**kwargs)
            self.labeledResamplers.append(labeledResampler)
            self.unlabeledResamplers.append(unlabeledResampler)
        labeledPosCount = np.zeros(kmeans.n_clusters).astype(int)
        labeledNegCount = np.zeros(kmeans.n_clusters).astype(int)
        unlabeledPosCount = np.zeros(kmeans.n_clusters).astype(int)
        unlabeledNegCount = np.zeros(kmeans.n_clusters).astype(int)

# Cell
import numpy as np
def getGroups(L):
    mapp = {g:i for i,g in enumerate(np.unique(L))}
    groups = np.array([mapp[g] for g in L])
    return groups

# Cell

class SyntheticPool(StructuredPool):
    def __init__(self,*args,**kwargs):
        self.pos_components = kwargs["pos_components"]
        self.neg_components = kwargs["neg_components"]
        self.component_assignments = kwargs["component_assignments"].astype(int)
        super().__init__(kwargs["x"],kwargs["y"],kwargs["sgid"],
                         np.arange(len(kwargs["x"])),kwargs["maxSGID"])
        self.initParameters()

    def initParameters(self):
        components = np.unique(self.component_assignments)
        n_components = components.max() + 1
        try:
            self.eta = np.zeros(n_components)
        except TypeError:
            print("n_components found: ",n_components)
            raise
        self.gamma = np.zeros(n_components)
        self.pi = np.zeros(n_components)
        self.rho = np.zeros(n_components)
        for k in components:
            componentMask = self.component_assignments == k
            self.gamma[k] = componentMask.sum()
            self.eta[k] = self.y[componentMask].sum() / componentMask.sum()
            self.pi[k] = componentMask[self.y].sum()
            self.rho[k] = componentMask[~self.y].sum()
        self.gamma /= self.gamma.sum()
        self.pi /= self.pi.sum()
        self.rho /= self.rho.sum()
        self.alpha = self.y.sum() / len(self.y)


    def __add__(self,b):
        x = np.concatenate([self.x, b.x])
        y = np.concatenate([self.y,b.y])
        component_assignments = np.concatenate([self.component_assignments,
                                                b.component_assignments])
        sgid = np.concatenate((self.sgid,b.sgid))
        maxSGID = max(self.maxSGID,b.maxSGID)

        kwargs = {"pos_components":self.pos_components,
                  "neg_components": self.neg_components,
                  "x": x, "y": y, "component_assignments": component_assignments,
                  "sgid": sgid, "maxSGID": maxSGID}
        return SyntheticPool(**kwargs)

    def get(self,idx):
        kwargs = {"pos_components":self.pos_components, "neg_components": self.neg_components,
                  "x": self.x[idx], "y": self.y[idx], "component_assignments": self.component_assignments[idx],
                  "sgid": self.sgid[idx], "maxSGID": self.maxSGID}
        return SyntheticPool(**kwargs)

    @staticmethod
    def sampleData(pos_components,neg_components, eta, gamma, num_points):
        xs,ys,cs = [],[],[]
        for j,(fj1,fj0,eta_j,gamma_j) in enumerate(zip(pos_components,neg_components,eta,gamma)):
            N = num_points * gamma_j
            nPos = round(N * eta_j)
            nNeg = round(N * (1 - eta_j))
            if nPos:
                xs.append(fj1.rvs(nPos).reshape((nPos,-1)))
                ys.append(np.ones(nPos).astype(bool))
            if nNeg:
                xs.append(fj0.rvs(nNeg).reshape((nNeg,-1)))
                ys.append(np.zeros(nNeg).astype(bool))
            cs.append([j] * (nPos + nNeg))
        return np.concatenate(xs), np.concatenate(ys),np.concatenate(cs)

    def __repr__(self):
        return "alpha:{}\npi:{}\nrho:{}\neta: {}\ngamma:{}".format(self.alpha,
                                                                   self.pi,
                                                                   self.rho,
                                                                   self.eta,self.gamma)

    def classConditional(self,idx):
        f1s = np.zeros(len(idx))
        f0s = np.zeros(len(idx))
        for i in idx:
            k = self.component_assignments[i]
            f1s[i] = self.pos_components[k].pdf(self.x[i])
            f0s[i] = self.neg_components[k].pdf(self.x[i])
        return f1s,f0s

    def posterior(self,idx):
        f1, f0 = self.classConditional(idx)
        alpha = np.array([self.eta[self.component_assignments[i]] for i in idx]).ravel()
        return alpha * f1 / (alpha * f1 + (1-alpha) * f0)

    @staticmethod
    def fromParameterization2(eta,gamma,pos_components,neg_components,num_points,partitionNum,maxSGID):
        x,y,component_assignments = SyntheticPool.sampleData(pos_components,
                                                            neg_components,
                                                            eta,gamma,
                                                            num_points)
        return SyntheticPool(pos_components=pos_components,
                             neg_components=neg_components,
                             x=x,y=y,component_assignments=component_assignments,
                             sgid=np.ones(len(x)).astype(int) * partitionNum,
                             maxSGID=maxSGID)

    @staticmethod
    def fromParameterization1(alpha,pi,rho,pos_components,neg_components,num_points,partitionNum,maxSGID):
        gamma = alpha * pi + (1 - alpha) * rho
        eta = alpha * pi / gamma
        x,y,component_assignments = SyntheticPool.sampleData(pos_components,
                                                            neg_components,
                                                            eta,gamma,
                                                            num_points)
        return SyntheticPool(pos_components=pos_components,
                             neg_components=neg_components,
                             x=x,y=y,component_assignments=component_assignments,
                             sgid=np.ones(len(x)).astype(int) * partitionNum,
                             maxSGID=maxSGID)

# Cell
import typing
import numpy as np
from sklearn.metrics import roc_auc_score
from .datagen.data.syntheticDistributions import generateComponents
import abc
from easydict import EasyDict
from sklearn.cluster import KMeans
from tqdm import tqdm,trange

class SyntheticDatasetBase(object):
    __metaclass__ = abc.ABCMeta
    def __init__(self,labeledSamples,unlabeledSamples):
        self.labeledSamples = labeledSamples
        self.unlabeledSamples = unlabeledSamples
        self.makeClusterer()
        super(SyntheticDatasetBase,self).__init__()

    def makeClusterer(self):
        clusterDict = {}
        for l in self.labeledSamples + self.unlabeledSamples:
            for xi,ci in zip(l.x,l.component_assignments):
                s = xi.tobytes()
                if s not in clusterDict:
                    clusterDict[s] = ci
                elif clusterDict[s] != ci:
                    raise ValueError("Found point where clusters overlap")
        def predictor(x):
            if not len(x[0].shape):
                return clusterDict[x.tobytes()]
            return np.array([clusterDict[xi.tobytes()] for xi in x])
        n_clusters = len(np.unique(list(clusterDict.values())))
        self.trueClusterer = EasyDict({"predict":predictor,"n_clusters":n_clusters})
        X = np.concatenate([l.x for l in self.labeledSamples + self.unlabeledSamples])
        self.bootstrappedSample = X[np.random.randint(0,len(X),size=len(X))]

    @classmethod
    def from_criteria(cls,n_targets: int, n_clusters: int, dim: int,
                      aucRange: typing.Iterable[float],
                      irreducibility_range: typing.Iterable[float],
                      num_points_labeled_partition: typing.Callable,
                      num_points_unlabeled_partition: typing.Callable,
                      timeoutMins : float, nTimeouts : int):
        print("generating components")
        NMix = generateComponents(n_clusters,dim,aucRange,irreducibility_range=irreducibility_range,
                                  timeoutMins=timeoutMins,nTimeouts=nTimeouts)
        print("done generating components")
        d = cls.from_components(n_targets,
                                             NMix.dg.components_pos,
                                             NMix.dg.components_neg,
                                             num_points_labeled_partition,
                                             num_points_unlabeled_partition)
        d.NMix = NMix
        return d

    @classmethod
    @abc.abstractmethod
    def sampleParameters(cls,n_partitions,n_components):
        raise NotImplementedError()

    @classmethod
    def from_components(cls, n_partitions: int, pos_components: typing.Iterable,
                        neg_components: typing.Iterable,
                        num_points_labeled_partition: typing.Callable,
                        num_points_unlabeled_partition: typing.Callable,failCount=0):
        labeledSamples = [None for _ in range(n_partitions)]
        unlabeledSamples = [None for _ in range(n_partitions)]
        labeledClusterPosCount = np.zeros(len(pos_components)).astype(int)
        labeledClusterNegCount = np.zeros(len(neg_components)).astype(int)
        labeledSampleParameters, unlabeledSampleParameters = cls.sampleParameters(n_partitions,len(pos_components))
        # Make Partitions
        instanceCounter = 0
        for partitionNum in trange(n_partitions):
            n_labeled_i = num_points_labeled_partition()
            n_unlabeled_i = num_points_unlabeled_partition()
            labeledSamples[partitionNum] = SyntheticPool.fromParameterization2(*labeledSampleParameters[partitionNum],
                                                                                  pos_components,neg_components,
                                                                                  n_labeled_i,partitionNum,n_partitions-1)
            labeledSamples[partitionNum].instanceNum += instanceCounter
            instanceCounter = labeledSamples[partitionNum].instanceNum.max() + 1
            for k in trange(len(neg_components),leave=False):
                clusterMask = labeledSamples[partitionNum].component_assignments == k
                labeledClusterPosCount[k] += labeledSamples[partitionNum].y[clusterMask].sum()
                labeledClusterNegCount[k] += clusterMask.sum() - labeledSamples[partitionNum].y[clusterMask].sum()
            unlabeledSamples[partitionNum] = SyntheticPool.fromParameterization2(*unlabeledSampleParameters[partitionNum],
                                                                                  pos_components,neg_components,
                                                                                  n_unlabeled_i,partitionNum,n_partitions-1)
            unlabeledSamples[partitionNum].instanceNum += instanceCounter
            instanceCounter =unlabeledSamples[partitionNum].instanceNum.max() + 1
        if (labeledClusterPosCount == 0).any() or (labeledClusterNegCount == 0).any():
            if failCount < 10:
                print("Retrying sample generation, repeat ",failCount + 1)
                return cls.from_components(n_partitions,pos_components,neg_components,
                                          num_points_labeled_partition,
                                           num_points_unlabeled_partition,
                                           failCount=failCount+1)
            else:
                raise ValueError("Failed to create labeled clusters with both classes 5 times in a row")
        print("done from components")
        return cls(labeledSamples,unlabeledSamples)

# Cell
class SyntheticSetting1(SyntheticDatasetBase):

    @classmethod
    def sampleParameters(cls,n_partitions,n_components):
        eta = np.random.uniform(0.01,0.99,size=n_components)
        gamma = np.random.dirichlet(np.ones(n_components)*2)
        labeledEta = np.tile(eta,(n_partitions,1))
        labeledGamma = np.tile(gamma,(n_partitions,1))
        unlabeledEta = np.tile(eta,(n_partitions,1))
        unlabeledGamma = np.tile(gamma,(n_partitions,1))
        return list(zip(labeledEta,labeledGamma)), list(zip(unlabeledEta,unlabeledGamma))

# Cell
class SyntheticSetting2(SyntheticDatasetBase):

    @classmethod
    def sampleParameters(cls,n_partitions,n_components):
        labeledEta = np.random.uniform(0.01,0.99,size=(n_partitions,n_components))
        unlabeledEta = np.random.uniform(0.01,0.99,size=(n_partitions,n_components))
        labeledGamma = np.random.dirichlet(np.ones(n_components)*2,size=n_partitions)
        unlabeledGamma = np.random.dirichlet(np.ones(n_components)*2,size=n_partitions)
        return list(zip(labeledEta,labeledGamma)), list(zip(unlabeledEta,unlabeledGamma))

# Cell
import folktables as ft
import os
import pandas as pd
from tqdm import trange,tqdm
from glob import glob
import numpy as np

class ACSLoader(object):
    def __init__(self,*args,**kwargs):
        super(ACSLoader,self).__init__(*args,**kwargs)
    @classmethod
    def getData(*args,**kwargs):
        survey_year = kwargs.get("survey_year","2018")
        horizon = kwargs.get("horizon","5-Year")
        survey = kwargs.get("survey","person")
        root_dir = kwargs.get("root_dir","data")
        state_list = kwargs.get("state_list",ft.state_list)
        problem_name = kwargs.get("problem_name","income")
        problem_map = {"income": ft.ACSIncome,
                       "employment": ft.ACSEmployment,
                       "health_insurance": ft.ACSHealthInsurance,
                       "public_coverage": ft.ACSPublicCoverage,
                       "travel_time": ft.ACSTravelTime,
                       "mobility": ft.ACSMobility,
                       "employment_filtered": ft.ACSEmploymentFiltered,
                       "income_poverty_ratio": ft.ACSIncomePovertyRatio}
        VALID_PROBLEMS = set(list(problem_map.keys()))
        assert problem_name in VALID_PROBLEMS, f"problem name must be one of: {VALID_PROBLEMS}"
        try:
#             acs_data = pd.read_parquet(os.path.join(root_dir,f"acs_{survey_year}_{horizon}_{survey}.parquet"))
            acs_states = [pd.read_parquet(filepath) for filepath in tqdm(sorted(glob(os.path.join(root_dir,f"acs_{survey_year}_{horizon}_{survey}_*.parquet"))))]
        except FileNotFoundError:
            print("cannot find parquet, loading/downloading csv...")
            datasource = ft.ACSDataSource(survey_year=survey_year,
                                          horizon=horizon,
                                          survey=survey,
                                 root_dir=root_dir)
            print("loading data frame")
            acs_data = datasource.get_data(states=state_list,download=True,)
            acs_grouped = acs_data.groupby("ST",as_index=False)
            for state,statedf in tqdm(acs_grouped):
                statedf.to_parquet(os.path.join(root_dir,f"acs_{survey_year}_{horizon}_{survey}_{state}.parquet"))
            acs_states = [y for (x,y) in acs_grouped]
        print("mapping states to problem")
        states = [problem_map[problem_name].df_to_numpy(state_df) for state_df in tqdm(acs_states)]
        state_ids = [np.ones(len(s[0])) * state_num for state_num,s in enumerate(states)]
        print("creating matrices")
        X = np.concatenate([s[0] for s in states])
        y = np.concatenate([s[1] for s in states])
        print(f"{X.shape[0]} total instances")
        state_ids = np.concatenate(state_ids)
        return X,y,state_ids

    def makeStructuredDataPool(self,*args,**kwargs):
        X,y,state_ids = ACSLoader.getData(*args,**kwargs)
        self.structuredPool = StructuredPool(X,y,state_ids,np.arange(len(X)),state_ids[-1])

# export
class ACSLoaderSetting1(RandomSplitLoader,Setting1Loader,ACSLoader,DataLoader):
     def __init__(self,*args,**kwargs):
        super(ACSLoaderSetting1,self).__init__(*args,**kwargs)

class ACSLoaderSetting2(RandomSplitLoader,Setting2Loader,ACSLoader,DataLoader):
     def __init__(self,*args,**kwargs):
        super(ACSLoaderSetting2,self).__init__(*args,**kwargs)


# export
import torch
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import OneHotEncoder
from datasets import load_dataset
from sklearn.decomposition import PCA
import pickle

class HuggingfaceDataset(object):
    @classmethod
    def getData(*args,**kwargs):
        dsname = kwargs.get("dataset_name","amazon_reviews_multi")
        dataset = load_dataset(dsname, "all_languages",
                      cache_dir=kwargs.get("cache_dir","/data/dzeiberg/huggingface/"))
        tr = dataset["train"].to_pandas()
        try:
            trainEncodings = torch.load(f"/data/dzeiberg/leveragingStructureResponseExperiments/{dsname}/trainEncodings.pt")
        except FileNotFoundError:
            print("Could not load embeddings, re-processing...")
            model = SentenceTransformer('all-MiniLM-L6-v2')
            trainEncodings = model.encode(tr.review_body,
             show_progress_bar=True,device=kwargs.get("device","cuda:3"),
                              batch_size=kwargs.get("batch_size",64),)
            torch.save(f"/data/dzeiberg/leveragingStructureResponseExperiments/{dsname}/trainEncodings.pt")
        X,y = trainEncodings,(tr.stars > kwargs.get("starThreshold",3)).values
        uniqueGroups = list(tr.product_category.unique())
        groupMap = dict(zip(uniqueGroups, np.arange(len(uniqueGroups))))
        group = np.zeros((X.shape[0]))
        for i,g in enumerate(tr.product_category):
            group[i] = groupMap[g]
        if kwargs.get("pca",False):
            try:
                with open(f"/data/dzeiberg/leveragingStructureResponseExperiments/{dsname}/pca.skl.pkl","rb") as f:
                    pca = pickle.load(f)    
            except FileNotFoundError:
                pca = PCA(n_components=25)
                pca.fit(X)
                with open(f"/data/dzeiberg/leveragingStructureResponseExperiments/{dsname}/pca.skl.pkl","wb") as f:
                    pickle.dump(pca,f)
            X = pca.transform(X)
        return X,y,group
    
    def makeStructuredDataPool(self,*args,**kwargs):
        X,y,group_ids = HuggingfaceDataset.getData(*args,**kwargs)
        self.structuredPool = StructuredPool(X,y,group_ids,
                                             np.arange(len(X)),
                                             max(group_ids))
    
class HuggingfaceDatasetSetting1(RandomSplitLoader,Setting1Loader,HuggingfaceDataset,DataLoader):
     def __init__(self,*args,**kwargs):
        super(HuggingfaceDatasetSetting1,self).__init__(*args,**kwargs)

class HuggingfaceDatasetSetting2(RandomSplitLoader,Setting2Loader,HuggingfaceDataset,DataLoader):
     def __init__(self,*args,**kwargs):
        super(HuggingfaceDatasetSetting2,self).__init__(*args,**kwargs)
